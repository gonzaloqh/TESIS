{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from googletrans) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (2024.2.2)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install translate\n",
    "!pip install googletrans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_score(features):\n",
    "\n",
    "    #features_PPDB2.0Score: Este puntaje representa la confianza general en la calidad del par de paráfrasis según PPDB 2.0. Un valor más alto indica una mayor probabilidad de que las frases sean paráfrasis válidas.\n",
    "    #features_AGigaSim: Mide la similitud distribucional entre las frases basándose en contextos observados en el corpus Annotated Gigaword. Valores más altos sugieren una mayor similitud semántica.\n",
    "    #features_GoogleNgramSim: Calcula la similitud entre las frases según contextos observados en el corpus de Google Ngram. Al igual que AGigaSim, valores más altos indican mayor similitud semántica.\n",
    "    #features_Equivalence: Predice la probabilidad de que el par de frases represente una equivalencia semántica (es decir, que ambas frases se implican mutuamente). Un valor cercano a 1 sugiere una fuerte equivalencia, mientras que un valor cercano a 0 indica lo contrario.\n",
    "    #features_Exclusion: Predice la probabilidad de que las frases se excluyan mutuamente en términos semánticos, lo que podría indicar una contradicción. Valores más altos señalan una mayor probabilidad de exclusión.\n",
    "    #features_ForwardEntailment: Indica la probabilidad de que la primera frase implique a la segunda. Un valor alto sugiere que la primera frase contiene o implica el significado de la segunda.\n",
    "    #features_Independent: Predice la probabilidad de que las frases sean semánticamente independientes, es decir, que no tengan relación significativa.\n",
    "    #features_WordCountDiff y features_CharCountDiff: Miden la diferencia en el número de palabras y caracteres entre las frases, respectivamente. Diferencias significativas pueden sugerir variaciones en la complejidad o estructura, aunque no necesariamente en el significado.\n",
    "\n",
    "    # Asignar pesos a las métricas según su relevancia\n",
    "    # Calcular la similitud como una combinación ponderada de las métricas\n",
    "    sum = 0.0\n",
    "    metrics = 0\n",
    "\n",
    "    if features.get('features_PPDB2.0Score'):\n",
    "        min_score = 3\n",
    "        max_score = 8\n",
    "        ppdb_score_normalized = (features['features_PPDB2.0Score'] - min_score) / (max_score - min_score)\n",
    "        #print(\"sumando: features.get('ppdb_score_normalized') \", ppdb_score_normalized)\n",
    "        sum += ppdb_score_normalized\n",
    "        metrics += 1\n",
    "    if features.get('features_AGigaSim'):\n",
    "        #print(\"sumando: features.get('features_AGigaSim') \", features.get('features_AGigaSim'))\n",
    "        sum += features.get('features_AGigaSim')\n",
    "        metrics += 1\n",
    "    if features.get('features_Equivalence'):\n",
    "        #print(\"sumando: features.get('features_Equivalence') \", features.get('features_Equivalence'))\n",
    "        sum += features.get('features_Equivalence')\n",
    "        metrics += 1\n",
    "    if features.get('features_Exclusion'):\n",
    "        #print(\"sumando: features.get('features_Exclusion') \", 1-features.get('features_Exclusion'))\n",
    "        sum += (1- features.get('features_Exclusion'))\n",
    "        metrics += 1\n",
    "    if features.get('features_Independent'):\n",
    "        #print(\"sumando: features.get('features_Independent') \", 1-features.get('features_Independent'))\n",
    "        sum += (1 - features.get('features_Independent'))\n",
    "        metrics += 1\n",
    "\n",
    "    response = sum/metrics\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_features(features_str):\n",
    "    features = {}\n",
    "    for item in features_str.split():\n",
    "        if \"=\" in item:  # Asegurar que el formato sea clave=valor\n",
    "            key, value = item.split(\"=\", 1)  # Dividir en clave y valor\n",
    "            # Manejar valores \"NA\" convirtiéndolos en None, o convertir a float si es posible\n",
    "            if value == \"NA\":\n",
    "                value = None\n",
    "            else:\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    pass  # Mantener como string si no se puede convertir\n",
    "            # Agregar al diccionario con el prefijo 'features_'\n",
    "            features[f\"features_{key}\"] = value\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def get_spanish_traduction(prompt):\n",
    "    \"\"\"\n",
    "    Envía un prompt a Phi para obtener hasta 5 sinónimos de una palabra.\n",
    "\n",
    "    Args:\n",
    "        word (str): La palabra para la cual se necesitan sinónimos.\n",
    "\n",
    "    Returns:\n",
    "        list: Un array con los sinónimos proporcionados por Phi.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://127.0.0.1:11434/api/generate\",\n",
    "        headers=headers,\n",
    "        json={\"model\": \"phi3.5\", \"prompt\": prompt}\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Procesa la respuesta en múltiples líneas\n",
    "        all_responses = []\n",
    "        for line in response.text.splitlines():\n",
    "            # Convierte cada línea a un diccionario y extrae la clave \"response\"\n",
    "            try:\n",
    "                data = json.loads(line)  # Utiliza json.loads en lugar de requests.utils.json\n",
    "                if 'response' in data:\n",
    "                    all_responses.append(data['response'])\n",
    "            except ValueError:\n",
    "                # En caso de que una línea no sea JSON válido, continuar\n",
    "                continue\n",
    "\n",
    "        # Une todos los fragmentos en una sola cadena\n",
    "        return ''.join(all_responses)\n",
    "    else:\n",
    "        # Imprime el código de error si la solicitud falla\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "\n",
    "def traduccion_simple_phi(word):\n",
    "    prompt = (\n",
    "        f\"Proporciona la traducción real en español latino para el texto o frase \\\"{word}\\\". \"\n",
    "        \"Responde solo con un array de un elemento con la respuesta. Sin adicionales.\"\n",
    "    )\n",
    "    print(prompt)\n",
    "    try:\n",
    "        response = json.loads(get_spanish_traduction(prompt))\n",
    "        print(response[0])\n",
    "        return response[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        time.sleep(2)  # Pausa de 2 segundos\n",
    "\n",
    "def traduccion_sinonimo_phi(word):\n",
    "    prompt = (\n",
    "        f\"Proporciona un array con hasta 5 sinónimos más cómunes(en español) de una sola palabra para \\\"{word}\\\". \"\n",
    "        \"Si encuentras menos de 5 o ningno, incluye solo los disponibles o ninguno. Responde solo con el array.\"\n",
    "    )\n",
    "    return get_spanish_traduction(prompt)\n",
    "\n",
    "# Ejemplo de uso\n",
    "#synonyms = traduccion_simple(\"why are you laughing ?\")\n",
    "#print(\"Sinónimos:\", synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import time\n",
    "import re\n",
    "\n",
    "def traduccion_simple_local(word):\n",
    "    try:\n",
    "        if not word:\n",
    "            #print(\"La palabra de entrada es vacía o inválida.\")\n",
    "            return None\n",
    "\n",
    "        translator = Translator()\n",
    "        traduccion = translator.translate(word, src='en', dest='es')\n",
    "\n",
    "        if traduccion.text:\n",
    "            #print(\"traduciendo: \", word , \" a: \", traduccion.text)\n",
    "            return traduccion.text\n",
    "        else:\n",
    "            #print(\"No se pudo obtener la traducción.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al traducir: {e}\")\n",
    "        print(\"Intentando de nuevo sin caracteres especiales: \")\n",
    "\n",
    "        try:\n",
    "            translator = Translator()\n",
    "            translator.raise_exception = True\n",
    "            traduccion = translator.translate(re.sub(r'[^a-zA-Z0-9]', ' ', word), src='en', dest='es')\n",
    "\n",
    "            if traduccion.text:\n",
    "                #print(\"traduciendo: \", re.sub(r'[^a-zA-Z0-9]', ' ', word) , \" a: \", traduccion.text)\n",
    "                return traduccion.text\n",
    "            else:\n",
    "                print(\"No se pudo obtener la traducción.\")\n",
    "                return word\n",
    "        except Exception as f:\n",
    "            print(\"Segundo intento fallido\", {f})\n",
    "            return word\n",
    "    #finally:\n",
    "        # Siempre esperamos un segundo antes de continuar\n",
    "        #time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gonzalo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gonzalo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_antonyms(word):\n",
    "    antonyms = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())\n",
    "    return antonyms\n",
    "\n",
    "# Procesar una línea del archivo\n",
    "def process_line(line):\n",
    "    #print(\"procesando linea: \", line)\n",
    "    parts = line.strip().split(\" ||| \")\n",
    "    lhs = parts[0]  # Etiqueta gramatical\n",
    "    phrase = parts[1]  # Frase original\n",
    "    paraphrase = parts[2]  # Paráfrasis\n",
    "    features = parts[3]  # Características\n",
    "    parsed_features = parse_features(features)\n",
    "    score = calculate_score(parsed_features)\n",
    "    alignment = parts[4]  # Alineación\n",
    "    entailment = parts[5]  # Relación de inferencia (e.g., Equivalence, ForwardEntailment)\n",
    "\n",
    "    if any(char in (phrase or \"\") for char in [\"[\", \"]\"]) or any(char in (paraphrase or \"\") for char in [\"[\", \"]\"]):\n",
    "        return None\n",
    "\n",
    "    # Extraer los antónimos para crear pares contradictorios\n",
    "    phrase_words = phrase.split()\n",
    "    contradictory_phrases = []\n",
    "    for word in phrase_words:\n",
    "        antonyms = get_antonyms(word)\n",
    "        for antonym in antonyms:\n",
    "            contradictory_phrases.append(phrase.replace(word, antonym))\n",
    "\n",
    "    #print(\"contra_array: \", contradictory_phrases) #['concentrate', 'centralize', 'centralized']\n",
    "    \n",
    "    return {\n",
    "        \"lhs\": lhs,\n",
    "        #\"phrase\": traduccion_simple_local(phrase),\n",
    "        #\"paraphrase\": traduccion_simple_local(paraphrase),\n",
    "        \"phrase\": phrase,\n",
    "        \"paraphrase\": paraphrase,\n",
    "        \"score\": score,\n",
    "        \"contradictions\": contradictory_phrases, #['concentrate', 'centralize', 'centralized']\n",
    "        \"alignment\": alignment,\n",
    "        \"entailment\": entailment,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del Conteo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivo: 100%|██████████| 9103493/9103493 [11:32<00:00, 13137.84línea/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_ppdb_file(file_path):\n",
    "    num_filas=35\n",
    "    limit = False\n",
    "    i = 0\n",
    "    corpus = {\"paraphrases\": [], \"contradictions\": []}\n",
    "    # Obtener la cantidad total de líneas en el archivo\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        total_lines = sum(1 for _ in file)\n",
    "    print(\"Fin del Conteo\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        with tqdm(total=total_lines, desc=\"Procesando archivo\", unit=\"línea\") as pbar:\n",
    "            for line in file:\n",
    "                #Excepciones\n",
    "                if \"www\" in line:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                data = process_line(line)\n",
    "                if(data == None):\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                corpus[\"paraphrases\"].append((data[\"phrase\"], data[\"paraphrase\"],  data[\"entailment\"], data[\"score\"]))\n",
    "                for contradiction in data[\"contradictions\"]:\n",
    "#                    corpus[\"contradictions\"].append((data[\"phrase\"], traduccion_simple_local(contradiction), \"contradiction\", 1 - data[\"score\"]))\n",
    "                    corpus[\"contradictions\"].append((data[\"phrase\"], contradiction, \"contradiction\", 1 - data[\"score\"]))\n",
    "\n",
    "                if i >= num_filas and limit:\n",
    "                    break\n",
    "                i = i +1\n",
    "                pbar.update(1)\n",
    "    return corpus\n",
    "\n",
    "ppdb_file = \"ppdb-2.0-m-all\"\n",
    "corpus = process_ppdb_file(ppdb_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def export_to_tsv(data, file_name):\n",
    "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter=\"\\t\")\n",
    "        writer.writerow([\"Phrase\", \"Paraphrase/Contradiction\", \"Type\", \"Score\"])  # Encabezados\n",
    "        for phrase, related, type, score in data:\n",
    "            writer.writerow([phrase, related, type, score])\n",
    "\n",
    "export_to_tsv(corpus[\"paraphrases\"], \"paraphrases.tsv\")\n",
    "export_to_tsv(corpus[\"contradictions\"], \"contradictions.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
