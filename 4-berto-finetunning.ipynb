{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gonzalo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 437.8 kB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.7/57.7 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "     ---------------------------------------- 0.0/71.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 71.4/71.4 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/9.7 MB 2.8 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/9.7 MB 4.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/9.7 MB 4.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/9.7 MB 4.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/9.7 MB 4.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.7 MB 5.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.8/9.7 MB 4.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.7 MB 5.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.3/9.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/9.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.1/9.7 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.7 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.6/9.7 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/9.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.7 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.7 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.6/9.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.8/9.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.7 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.4/9.7 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.7/9.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.3/9.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.7/9.7 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.7 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.0/9.7 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.3/9.7 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.7 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "   ---------------------------------------- 0.0/480.6 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 389.1/480.6 kB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 480.6/480.6 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 30.7/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 179.3/179.3 kB 10.6 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.12-cp312-cp312-win_amd64.whl (437 kB)\n",
      "   ---------------------------------------- 0.0/437.9 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 368.6/437.9 kB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 437.9/437.9 kB 6.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "   ---------------------------------------- 0.0/464.1 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 399.4/464.1 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 464.1/464.1 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "   ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 146.7/146.7 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading pyarrow-19.0.0-cp312-cp312-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.2 MB 9.4 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.6/25.2 MB 9.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.9/25.2 MB 7.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.3/25.2 MB 7.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.5/25.2 MB 7.3 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.9/25.2 MB 7.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.2/25.2 MB 7.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.4/25.2 MB 6.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.8/25.2 MB 6.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.0/25.2 MB 6.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.3/25.2 MB 6.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.7/25.2 MB 6.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.0/25.2 MB 6.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.2/25.2 MB 6.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.4/25.2 MB 6.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.6/25.2 MB 6.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.6/25.2 MB 6.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.6/25.2 MB 5.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.8/25.2 MB 5.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.1/25.2 MB 5.6 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.5/25.2 MB 5.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.8/25.2 MB 5.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.1/25.2 MB 5.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.5/25.2 MB 5.8 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.6/25.2 MB 5.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 5.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.2/25.2 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.6/25.2 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.9/25.2 MB 6.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.0/25.2 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.4/25.2 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.9/25.2 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.0/25.2 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.5/25.2 MB 6.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.9/25.2 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.9/25.2 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.9/25.2 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.9/25.2 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/25.2 MB 5.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.4/25.2 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.0/25.2 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.2/25.2 MB 5.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.3/25.2 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.7/25.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.2/25.2 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.6/25.2 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.8/25.2 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 13.1/25.2 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/25.2 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/25.2 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.5/25.2 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.6/25.2 MB 5.4 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.8/25.2 MB 5.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/25.2 MB 5.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.6/25.2 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.6/25.2 MB 4.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.2/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.6/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.8/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.9/25.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.4/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.7/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.3/25.2 MB 5.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.6/25.2 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.9/25.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 18.0/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.5/25.2 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.9/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.2/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.4/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.6/25.2 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.7/25.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.8/25.2 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.1/25.2 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.1/25.2 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.8/25.2 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.3/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.6/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.0/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.3/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.7/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.9/25.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/25.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/25.2 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.8/25.2 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/25.2 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.2/25.2 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.2 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.5/25.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/25.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.0/25.2 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.2 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 256.0/273.6 kB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 273.6/273.6 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 303.8/303.8 kB 9.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.4 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.4 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.4 MB 7.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.9/2.4 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 4.6 MB/s eta 0:00:00\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "   ---------------------------------------- 0.0/51.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 51.3/51.3 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.1/44.1 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "   ---------------------------------------- 0.0/90.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 90.4/90.4 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, networkx, multidict, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, torch, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 datasets-3.2.0 dill-0.3.8 filelock-3.17.0 frozenlist-1.5.0 fsspec-2024.9.0 huggingface-hub-0.28.1 multidict-6.1.0 multiprocess-0.70.16 networkx-3.4.2 propcache-0.2.1 pyarrow-19.0.0 regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 torch-2.6.0 tqdm-4.67.1 transformers-4.48.3 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"keras<3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0->transformers[torch]) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gonzq\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch] accelerate --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'accelerate\"\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install 'accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 995/995 [00:00<00:00, 1474.93 examples/s]\n",
      "Map: 100%|██████████| 111/111 [00:00<00:00, 1474.11 examples/s]\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\gonzq\\AppData\\Local\\Temp\\ipykernel_9224\\2284764894.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m\n\u001b[0;32m     76\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     77\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     78\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     86\u001b[0m \u001b[39m# 🚀 Iniciar el entrenamiento\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2172\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   2173\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   2174\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   2175\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   2176\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[39m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mno_sync, model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_samples) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2533\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3672\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   3674\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3675\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, num_items_in_batch\u001b[39m=\u001b[39;49mnum_items_in_batch)\n\u001b[0;32m   3677\u001b[0m \u001b[39mdel\u001b[39;00m inputs\n\u001b[0;32m   3678\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3679\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3680\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   3681\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3729\u001b[0m         loss_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_items_in_batch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3730\u001b[0m     inputs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3731\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   3732\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3733\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1461\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1453\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   1456\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1461\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1462\u001b[0m     input_ids,\n\u001b[0;32m   1463\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1464\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1465\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1466\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1467\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1468\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1469\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1470\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1471\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1472\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1473\u001b[0m )\n\u001b[0;32m   1475\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1476\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1143\u001b[0m     embedding_output,\n\u001b[0;32m   1144\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1145\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1146\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1147\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1148\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1149\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1150\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1151\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1152\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1153\u001b[0m )\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    696\u001b[0m         hidden_states,\n\u001b[0;32m    697\u001b[0m         attention_mask,\n\u001b[0;32m    698\u001b[0m         layer_head_mask,\n\u001b[0;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    701\u001b[0m         past_key_value,\n\u001b[0;32m    702\u001b[0m         output_attentions,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    586\u001b[0m         hidden_states,\n\u001b[0;32m    587\u001b[0m         attention_mask,\n\u001b[0;32m    588\u001b[0m         head_mask,\n\u001b[0;32m    589\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    590\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[0;32m    518\u001b[0m         head_mask,\n\u001b[0;32m    519\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    520\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    521\u001b[0m         past_key_value,\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\gonzq\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m is_causal \u001b[39m=\u001b[39m (\n\u001b[0;32m    437\u001b[0m     \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cross_attention \u001b[39mand\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m tgt_len \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    438\u001b[0m )\n\u001b[1;32m--> 440\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mscaled_dot_product_attention(\n\u001b[0;32m    441\u001b[0m     query_layer,\n\u001b[0;32m    442\u001b[0m     key_layer,\n\u001b[0;32m    443\u001b[0m     value_layer,\n\u001b[0;32m    444\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    445\u001b[0m     dropout_p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_prob \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39melse\u001b[39;49;00m \u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m    446\u001b[0m     is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[0;32m    447\u001b[0m )\n\u001b[0;32m    449\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    450\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mreshape(bsz, tgt_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📌 Cargar el corpus JSON\n",
    "with open(\"3-corpus_texto_estructurado.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "# 📌 Unir los textos en una sola lista\n",
    "text_data = [item for item in corpus]\n",
    "\n",
    "# 📌 Cargar el tokenizador de BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "# 📌 Tokenizar los datos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# 📌 Dividir datos en train (90%) y test (10%)\n",
    "train_texts, test_texts = train_test_split(text_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# 📌 Crear dataset Hugging Face\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\"text\": train_texts}),\n",
    "    \"test\": Dataset.from_dict({\"text\": test_texts})\n",
    "})\n",
    "\n",
    "# 📌 Tokenizar los datasets\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 📌 Cargar modelo BETO para MLM\n",
    "model = BertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "# 📌 Mover modelo a GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# 📌 Data Collator para MLM\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15  # 🔹 Enmascarar el 15% de las palabras\n",
    ")\n",
    "\n",
    "# 📌 Configurar entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./beto_pretrained\",\n",
    "    evaluation_strategy=\"epoch\",   # Evaluar al final de cada epoch\n",
    "    save_strategy=\"epoch\",         # Guardar modelo en cada epoch\n",
    "    per_device_train_batch_size=64, # Ajusta según tu GPU\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,            # Entrenar por 3 épocas\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",              # No enviar logs a WandB\n",
    "    push_to_hub=False,             # No subir a Hugging Face Hub\n",
    ")\n",
    "\n",
    "# 📌 Definir métricas de evaluación\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    mask = labels != -100  # Ignorar tokens enmascarados\n",
    "    accuracy = (predictions[mask] == labels[mask]).mean()\n",
    "\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# 📌 Entrenar el modelo con Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],  # Evaluación en conjunto de test\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 🚀 Iniciar el entrenamiento\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./beto_preentrenado\")\n",
    "tokenizer.save_pretrained(\"./beto_preentrenado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"./beto_preentrenado\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./beto_preentrenado\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE TUNING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📌 Dividir en 80% entrenamiento y 20% prueba\n",
    "train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
    "    questions, answers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 📌 Crear datasets para Hugging Face\n",
    "train_dataset = Dataset.from_dict({\"question\": train_questions, \"context\": train_answers})\n",
    "test_dataset = Dataset.from_dict({\"question\": test_questions, \"context\": test_answers})\n",
    "\n",
    "# 📌 Tokenizar\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# 📌 Cargar el corpus JSON de preguntas y respuestas\n",
    "with open(\"3-corpus_preguntas_respuestas.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    qa_corpus = json.load(f)\n",
    "\n",
    "# 📌 Extraer preguntas y respuestas en listas separadas\n",
    "questions = [pair[0] for pair in qa_corpus]\n",
    "answers = [pair[1] for pair in qa_corpus]\n",
    "\n",
    "# 📌 Cargar el tokenizador y el modelo preentrenado de BETO\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./beto_preentrenado\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "\n",
    "# 📌 Función para tokenizar preguntas y respuestas\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"question\"], examples[\"context\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# 📌 Crear dataset en formato Hugging Face\n",
    "dataset = Dataset.from_dict({\"question\": questions, \"context\": answers})\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 📌 Configurar parámetros de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./beto_finetuned_qa\",\n",
    "    evaluation_strategy=\"epoch\",   # Evaluar al final de cada epoch\n",
    "    save_strategy=\"epoch\",         # Guardar modelo en cada epoch\n",
    "    per_device_train_batch_size=8, # Ajusta según tu GPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,            # Entrenar por 3 épocas\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# 📌 Definir la función de pérdida y métricas\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = (predictions == labels).mean()  # 🔹 Precisión en preguntas\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,  # Ahora evaluamos en datos de prueba\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 🚀 Iniciar el fine-tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usamos la CrossEntropyLoss, que es la predeterminada en BertForQuestionAnswering. Se ajusta bien a QA, ya que el modelo debe predecir el índice de inicio y fin de la respuesta dentro del contexto.\n",
    "#\n",
    "#📌 Métricas incluidas:\n",
    "#✅ Precisión (Accuracy): Mide cuántas preguntas fueron respondidas correctamente.\n",
    "#✅ Loss (Pérdida): Evalúa la diferencia entre las respuestas predichas y reales.\n",
    "#✅ F1-score (Opcional): Podemos agregarlo para evaluar mejor la calidad de las respuestas.\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = (predictions == labels).mean()  \n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")  # Agregamos F1-score\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./beto_finetuned_qa\")\n",
    "tokenizer.save_pretrained(\"./beto_finetuned_qa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./beto_finetuned_qa\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./beto_finetuned_qa\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
